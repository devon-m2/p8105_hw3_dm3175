---
title: "p8105_hw3_dm3175"
author: "Devon Morgan"
date: "10/9/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)
library("dplyr")
library(viridis)
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)

```

# Problem 1

The BRFSS dataset includes data from the Behavioral Risk Factors Surveillance System from 2002-2010. This dataset is located in the `p8105.datasets` package. The same dataset was used in homework 2.  

## Import and Clean Dataset
Loaded and cleaned BRFSS data from `p9105.datasets` package: 

```{r load BRFSS Data}
brfss_data = p8105.datasets::brfss_smart2010 %>% 
    janitor::clean_names() %>%
    filter(topic == "Overall Health") %>% 
    mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
  
```

The following steps were taken to clean the dataset: 

*  Cleaned variable names using `janitor::clean_names`.

*  Verified variable types imported correctly.

*  Filtered data to only consider "Overall Health" topic.

*  Using the `factor` function, organized the responses as a factor ordered with 5 levels: `Excellent`, `Very good`, `Good`, `Fair`, and `Poor`. 

## Questions

*  **In 2002, which states were observed at 7 locations?**

Using a combination of `group_by` and `summarize` functions, it can be found that in 2002, the following states were observed at 7 locations. Note that other states were observed at less than or greater than 7 locations. 

```{r}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(count_locations = n_distinct(locationdesc)) %>% 
  filter(count_locations == 7) %>% 
  knitr::kable(digits = 1)
```


*  **Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**

The following spaghetti plot displays the number of locations in each state from 2002 to 2010. The states are differentiated by colors as presented in the legend. 

```{r number of locations in each state}
brfss_data %>%
  group_by(year, locationabbr) %>% 
  summarize(count_locations = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = count_locations, color = locationabbr)) + 
  geom_line(se = FALSE) +
  labs(
    title = "Number of Sampling Locations in Each State, 2002 to 2010",
    x = "Year",
    y = "Number of Distinct Sampling Locations",
    caption = "Data from the BRFSS 2010 Dataset"
  ) + 
  theme_bw()

```


*  **Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**

The following table displays the mean and standard deviation of the proportion of "Excellent" responses across locations in NY State. 

```{r}
brfss_data %>% 
  filter(locationabbr == "NY" & 
           (year == 2002 | year == 2006 | year == 2010) &
           response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_excellent_response = mean(data_value),
            sd_excellent_response = sd(data_value)) %>%
  knitr::kable(digits = 1)
```

*  **For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.**
*****FINISH*****

```{r}
brfss_data %>%
  group_by(year, locationabbr, response) %>% 
  summarize(mean_response = mean(data_value))

    
```


# Problem 2

The Instacart Online Grocery Shopping Dataset (Instacart) from 2017 contains data from an online grocery service that permits shopping from local stores online, delivering groceries quickly. The dataset includes online order data from more than 200,000 users.   

## Load dataset

```{r load instacart data}
instacart_data = p8105.datasets::instacart %>% 
    janitor::clean_names()
    
```

## Description of dataset

The instacart dataset 


### Variable Descriptions
The following is a list of all the variables within the dataset and corresponding descriptions for reference taken from the dataset description on http://p8105.com/dataset_instacart.html. 

*  `order_id`: order identifier
*  `product_id`: product identifier
*  `add_to_cart_order`: order in which each product was added to cart
*  `reordered`: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
*  `user_id`: customer identifier
*  `eval_set`: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
*  `order_number`: the order sequence number for this user (1=first, n=nth)
*  `order_dow`: the day of the week on which the order was placed
*  `order_hour_of_day`: the hour of the day on which the order was placed
*  `days_since_prior_order`: days since the last order, capped at 30, NA if order_number=1
*  `product_name`: name of the product
*  `aisle_id`: aisle identifier
*  `department_id`: department identifier
*  `aisle`: the name of the aisle
*  `department`: the name of the department


## Questions

*  **How many aisles are there, and which aisles are the most items ordered from?**

There are `r distinct(instacart_data, aisle, .keep_all = TRUE) %>% nrow()` aisles noted in the dataset. The top 10 most frequently used aisles are displayed in the table below. These represent the aisles where the most ordered items originate from. The top three aisles are **fresh vegetables, fresh fruits, and packaged vegetable fruits**. 

```{r aisle counting}
instacart_data %>% 
  group_by(aisle) %>% 
  summarize(distinct_aisle = n()) %>%
  filter(min_rank(desc(distinct_aisle)) < 11) %>% 
  arrange(desc(distinct_aisle)) %>% 
  knitr::kable(digits = 1)
```


* **Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it**
The following plot displays the number of items ordered in each aisle. 

```{r items ordered per aisle}
instacart_data %>%
  group_by(aisle) %>% 
  summarize(distinct_product = n_distinct(product_id)) %>%
  arrange(desc(distinct_product)) %>% 
  ggplot(aes(x = aisle, y = distinct_product), color = aisle) +
  geom_point(alpha = .5) +
  labs( 
    viridis::scale_color_viridis(
    name = "aisle", 
    discrete = TRUE
  ) )
  

```


* **Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**
The following tables shows the most popular item and corresponding frequency for how many times it was ordered in each of the following aisles: "baking ingredients", "dog food care", and "packaged vegetables fruits". 

```{r}
instacart_data %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>%
  summarize(n_product = n()) %>%
  filter(min_rank(desc(n_product)) < 2) %>%
  knitr::kable(digits = 1)
            
```


* **Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**

The following table shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. Data is presented for each day of the week with a value of "0" in `order_bow` assumed to represent "Sunday". 

```{r}
instacart_data %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 3)
```

# Problem 3

## Load dataset

```{r load NOAA dataset}
NOAA_data = p8105.datasets::ny_noaa %>% 
    janitor::clean_names()
```

## Description of dataset

The NOAA dataset contains data from the NOAA National Climatic Data Center retrieved on August 15, 2017. The dataset contains `r nrow(NOAA_data)` rows and `r ncol(NOAA_data)` columns. The earliest observation time point began in `r min(NOAA_data$date)` and the last date of observation was `r max(NOAA_data$date)`.  

The dataset contains the following key variables: 

*  `id`: ID of the weather center
*  `date`: Date of observation
*  `prcp`: Precipitation level (tenths of mm)
*  `snow`: Snowfall (mm)
*  `snwd`: Snow depth (mm)
*  `tmax`: Maximum temperature (tenths of degrees C)
*  `tmin`: Minimum temperature (tenths of degrees C)

Each of the weather stations collect a different combination of variables listed above. This means that there are a significant amount of missing values throughout the dataset. The missing value counts for each variable are as follows: 

*  Precipitation level (`prcp`) has `r sum(is.na(NOAA_data$prcp))` missing values. This represents `r sum(is.na(NOAA_data$prcp))/nrow(NOAA_data)` portion missing values for this variable out of the full dataset.  
*  Snowfall level (`snow`) has `r sum(is.na(NOAA_data$snow))` missing values. This represents `r sum(is.na(NOAA_data$snow))/nrow(NOAA_data)` portion missing values for this variable out of the full dataset. 
*  Snow depth (`snwd`) has `r sum(is.na(NOAA_data$snwd))` missing values. This represents `r sum(is.na(NOAA_data$snwd))/nrow(NOAA_data)` portion missing values for this variable out of the full dataset. 
*  Maximum temperature (`tmax`) has `r sum(is.na(NOAA_data$tmax))` missing values. This represents `r sum(is.na(NOAA_data$tmax))/nrow(NOAA_data)` portion missing values for this variable out of the full dataset. 
*  Minimum temperature (`tmin`) has `r sum(is.na(NOAA_data$tmin))` missing values. This represents `r sum(is.na(NOAA_data$tmin))/nrow(NOAA_data)` portion missing values for this variable out of the full dataset. 

The large number of missing values for `tmax` and `tmin` relative to the full size of the dataset make it difficult to track changes in temperature over time, and to get a clear sense of what the typical minimum and maximum temperature levels are at different stations. This similarly applies to the missing values for precipitation level, snowfall level and snow depth which make comparisons across stations and throughout time difficult. 


## Questions

* **Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?**

```{r data cleaning NOAA}
clean_NOAA_data = NOAA_data %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmin = as.double(tmin), 
         tmax = as.double(tmax),
         prcp = as.double(prcp), 
         snow = as.double(snow),
         snwd = as.double(snwd)) %>% 
  mutate(tmax_celsius = tmax/10,
         tmin_celsius = tmin/10)
  
  


```


* **Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?**

boxplot 

* **Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.**

density plot
