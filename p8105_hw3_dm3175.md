p8105\_hw3\_dm3175
================
Devon Morgan
10/9/2018

Problem 1
=========

The BRFSS dataset includes data from the Behavioral Risk Factors Surveillance System from 2002-2010. This dataset is located in the `p8105.datasets` package. The same dataset was used in homework 2.

Import and Clean Dataset
------------------------

Loaded and cleaned BRFSS data from `p9105.datasets` package:

``` r
brfss_data = p8105.datasets::brfss_smart2010 %>% 
    janitor::clean_names() %>%
    filter(topic == "Overall Health") %>% 
    mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
```

The following steps were taken to clean the dataset:

-   Cleaned variable names using `janitor::clean_names`.

-   Verified variable types imported correctly.

-   Filtered data to only consider "Overall Health" topic.

-   Using the `factor` function, organized the responses as a factor ordered with 5 levels: `Excellent`, `Very good`, `Good`, `Fair`, and `Poor`.

Questions
---------

-   **In 2002, which states were observed at 7 locations?**

Using a combination of `group_by` and `summarize` functions, it can be found that in 2002, the following states were observed at 7 locations. Note that other states were observed at less than or greater than 7 locations.

``` r
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(count_locations = n_distinct(locationdesc)) %>% 
  filter(count_locations == 7) %>% 
  knitr::kable(digits = 1)
```

| locationabbr |  count\_locations|
|:-------------|-----------------:|
| CT           |                 7|
| FL           |                 7|
| NC           |                 7|

-   **Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**

The following spaghetti plot displays the number of locations in each state from 2002 to 2010. The states are differentiated by colors as presented in the legend.

``` r
brfss_data %>%
  group_by(year, locationabbr) %>% 
  summarize(count_locations = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = count_locations, color = locationabbr)) + 
  geom_line(se = FALSE) +
  labs(
    title = "Number of Sampling Locations in Each State, 2002 to 2010",
    x = "Year",
    y = "Number of Distinct Sampling Locations",
    caption = "Data from the BRFSS 2010 Dataset"
  ) + 
  theme_bw()
```

    ## Warning: Ignoring unknown parameters: se

<img src="p8105_hw3_dm3175_files/figure-markdown_github/number of locations in each state-1.png" width="90%" />

-   **Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**

The following table displays the mean and standard deviation of the proportion of "Excellent" responses across locations in NY State.

``` r
brfss_data %>% 
  filter(locationabbr == "NY" & 
           (year == 2002 | year == 2006 | year == 2010) &
           response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_excellent_response = mean(data_value),
            sd_excellent_response = sd(data_value)) %>%
  knitr::kable(digits = 1)
```

|  year|  mean\_excellent\_response|  sd\_excellent\_response|
|-----:|--------------------------:|------------------------:|
|  2002|                       24.0|                      4.5|
|  2006|                       22.5|                      4.0|
|  2010|                       22.7|                      3.6|

-   **For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.** \*\*\*\*\*FINISH\*\*\*\*\*

``` r
brfss_data %>%
  group_by(year, locationabbr, response) %>% 
  summarize(mean_response = mean(data_value))
```

    ## # A tibble: 2,215 x 4
    ## # Groups:   year, locationabbr [?]
    ##     year locationabbr response  mean_response
    ##    <int> <chr>        <fct>             <dbl>
    ##  1  2002 AK           Excellent          27.9
    ##  2  2002 AK           Very good          33.7
    ##  3  2002 AK           Good               23.8
    ##  4  2002 AK           Fair                8.6
    ##  5  2002 AK           Poor                5.9
    ##  6  2002 AL           Excellent          18.5
    ##  7  2002 AL           Very good          30.9
    ##  8  2002 AL           Good               32.7
    ##  9  2002 AL           Fair               12.1
    ## 10  2002 AL           Poor                5.9
    ## # ... with 2,205 more rows

Problem 2
=========

The Instacart Online Grocery Shopping Dataset (Instacart) from 2017 contains data from an online grocery service that permits shopping from local stores online, delivering groceries quickly. The dataset includes online order data from more than 200,000 users.

Load dataset
------------

``` r
instacart_data = p8105.datasets::instacart %>% 
    janitor::clean_names()
```

Description of dataset
----------------------

The instacart dataset

### Variable Descriptions

The following is a list of all the variables within the dataset and corresponding descriptions for reference taken from the dataset description on <http://p8105.com/dataset_instacart.html>.

-   `order_id`: order identifier
-   `product_id`: product identifier
-   `add_to_cart_order`: order in which each product was added to cart
-   `reordered`: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
-   `user_id`: customer identifier
-   `eval_set`: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval\_set)
-   `order_number`: the order sequence number for this user (1=first, n=nth)
-   `order_dow`: the day of the week on which the order was placed
-   `order_hour_of_day`: the hour of the day on which the order was placed
-   `days_since_prior_order`: days since the last order, capped at 30, NA if order\_number=1
-   `product_name`: name of the product
-   `aisle_id`: aisle identifier
-   `department_id`: department identifier
-   `aisle`: the name of the aisle
-   `department`: the name of the department

Questions
---------

-   **How many aisles are there, and which aisles are the most items ordered from?**

There are 134 aisles noted in the dataset. The top 10 most frequently used aisles are displayed in the table below. These represent the aisles where the most ordered items originate from. The top three aisles are **fresh vegetables, fresh fruits, and packaged vegetable fruits**.

``` r
instacart_data %>% 
  group_by(aisle) %>% 
  summarize(distinct_aisle = n()) %>%
  filter(min_rank(desc(distinct_aisle)) < 11) %>% 
  arrange(desc(distinct_aisle)) %>% 
  knitr::kable(digits = 1)
```

| aisle                         |  distinct\_aisle|
|:------------------------------|----------------:|
| fresh vegetables              |           150609|
| fresh fruits                  |           150473|
| packaged vegetables fruits    |            78493|
| yogurt                        |            55240|
| packaged cheese               |            41699|
| water seltzer sparkling water |            36617|
| milk                          |            32644|
| chips pretzels                |            31269|
| soy lactosefree               |            26240|
| bread                         |            23635|

-   **Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it** The following plot displays the number of items ordered in each aisle.

``` r
instacart_data %>%
  group_by(aisle) %>% 
  summarize(distinct_product = n_distinct(product_id)) %>%
  arrange(desc(distinct_product)) %>% 
  ggplot(aes(x = aisle, y = distinct_product), color = aisle) +
  geom_point(alpha = .5) +
  labs( 
    viridis::scale_color_viridis(
    name = "aisle", 
    discrete = TRUE
  ) )
```

<img src="p8105_hw3_dm3175_files/figure-markdown_github/items ordered per aisle-1.png" width="90%" />

-   **Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.** The following tables shows the most popular item and corresponding frequency for how many times it was ordered in each of the following aisles: "baking ingredients", "dog food care", and "packaged vegetables fruits".

``` r
instacart_data %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>%
  summarize(n_product = n()) %>%
  filter(min_rank(desc(n_product)) < 2) %>%
  knitr::kable(digits = 1)
```

| aisle                      | product\_name                                 |  n\_product|
|:---------------------------|:----------------------------------------------|-----------:|
| baking ingredients         | Light Brown Sugar                             |         499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |          30|
| packaged vegetables fruits | Organic Baby Spinach                          |        9784|

-   **Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**

The following table shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. Data is presented for each day of the week with a value of "0" in `order_bow` assumed to represent "Sunday".

``` r
instacart_data %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 3)
```

| product\_name    |       0|       1|       2|       3|       4|       5|       6|
|:-----------------|-------:|-------:|-------:|-------:|-------:|-------:|-------:|
| Coffee Ice Cream |  13.774|  14.316|  15.381|  15.318|  15.217|  12.263|  13.833|
| Pink Lady Apples |  13.441|  11.360|  11.702|  14.250|  11.552|  12.784|  11.938|

Problem 3
=========

Load dataset
------------

``` r
NOAA_data = p8105.datasets::ny_noaa %>% 
    janitor::clean_names()
```

Description of dataset
----------------------

The NOAA dataset contains data from the NOAA National Climatic Data Center retrieved on August 15, 2017. The dataset contains 2595176 rows and 7 columns. The earliest observation time point began in 1981-01-01 and the last date of observation was 2010-12-31.

The dataset contains the following key variables:

-   `id`: ID of the weather center
-   `date`: Date of observation
-   `prcp`: Precipitation level (tenths of mm)
-   `snow`: Snowfall (mm)
-   `snwd`: Snow depth (mm)
-   `tmax`: Maximum temperature (tenths of degrees C)
-   `tmin`: Minimum temperature (tenths of degrees C)

Each of the weather stations collect a different combination of variables listed above. This means that there are a significant amount of missing values throughout the dataset. The missing value counts for each variable are as follows:

-   Precipitation level (`prcp`) has 145838 missing values. This represents 0.0561958 portion missing values for this variable out of the full dataset.
-   Snowfall level (`snow`) has 381221 missing values. This represents 0.146896 portion missing values for this variable out of the full dataset.
-   Snow depth (`snwd`) has 591786 missing values. This represents 0.2280331 portion missing values for this variable out of the full dataset.
-   Maximum temperature (`tmax`) has 1134358 missing values. This represents 0.4371025 portion missing values for this variable out of the full dataset.
-   Minimum temperature (`tmin`) has 1134420 missing values. This represents 0.4371264 portion missing values for this variable out of the full dataset.

The large number of missing values for `tmax` and `tmin` relative to the full size of the dataset make it difficult to track changes in temperature over time, and to get a clear sense of what the typical minimum and maximum temperature levels are at different stations. This similarly applies to the missing values for precipitation level, snowfall level and snow depth which make comparisons across stations and throughout time difficult.

Questions
---------

-   **Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?**

``` r
clean_NOAA_data = NOAA_data %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmin = as.double(tmin), 
         tmax = as.double(tmax),
         prcp = as.double(prcp), 
         snow = as.double(snow),
         snwd = as.double(snwd)) %>% 
  mutate(tmax_celsius = tmax/10,
         tmin_celsius = tmin/10)
```

-   **Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?**

boxplot

-   **Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.**

density plot
